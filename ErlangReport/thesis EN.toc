\contentsline {section}{\numberline {1}Overview}{3}{section.1}%
\contentsline {section}{\numberline {2}Implementation}{3}{section.2}%
\contentsline {subsection}{\numberline {2.1}Architecture}{3}{subsection.2.1}%
\contentsline {subsection}{\numberline {2.2}Scalability}{5}{subsection.2.2}%
\contentsline {subsubsection}{\numberline {2.2.1}How does your design ensure scalability? What is the best case and the worst case in terms of the mix of different user requests? When would the responsiveness decrease?}{5}{subsubsection.2.2.1}%
\contentsline {subsubsection}{\numberline {2.2.2}How do you ensure conceptual scalability to hundreds, thousands, or millions of users? Are there any conceptual bottlenecks left?}{6}{subsubsection.2.2.2}%
\contentsline {subsubsection}{\numberline {2.2.3} Where did you sacrifice data consistency to improve scalability? How does this decision improve scalability? In which case would the overall data consistency suffer most? }{6}{subsubsection.2.2.3}%
\contentsline {section}{\numberline {3}Evaluation}{6}{section.3}%
\contentsline {subsection}{\numberline {3.1}Experimental set-up}{6}{subsection.3.1}%
\contentsline {subsubsection}{\numberline {3.1.1}Which CPU, tools, platform, versions of software, etc. you used }{6}{subsubsection.3.1.1}%
\contentsline {subsubsection}{\numberline {3.1.2} All details that are necessary to put the results into context.}{7}{subsubsection.3.1.2}%
\contentsline {subsubsection}{\numberline {3.1.3}All the parameters that influenced the run-time behavior of Erlang.}{7}{subsubsection.3.1.3}%
\contentsline {subsection}{\numberline {3.2}Experimental methodology}{7}{subsection.3.2}%
\contentsline {subsubsection}{\numberline {3.2.1}How did you measure? E.g. using wall clock time or CPU time, at client side or server side.}{7}{subsubsection.3.2.1}%
\contentsline {subsubsection}{\numberline {3.2.2}How often did you repeat your experiments, and how did you evaluate the results?}{7}{subsubsection.3.2.2}%
\contentsline {subsection}{\numberline {3.3}Three experiments}{8}{subsection.3.3}%
\contentsline {subsubsection}{\numberline {3.3.1}What (dependent) variable(s) did you measure? For example: speed-up, latency, through-put, degree of inconsistency.}{8}{subsubsection.3.3.1}%
\contentsline {subsubsection}{\numberline {3.3.2}What (independent) parameter did you vary? For example: number of scheduler threads, number of server processes, users, messages, followers, number of simultaneous requests }{8}{subsubsection.3.3.2}%
\contentsline {subsubsection}{\numberline {3.3.3}Describe the load that you generated and other relevant parameters. What are the pro- portions of different requests? How many users/messages/followers per user does your system contain, how many clients are connected simultaneously, and how many requests are there per connection?}{8}{subsubsection.3.3.3}%
\contentsline {subsubsection}{\numberline {3.3.4}Results Experiment 1}{9}{subsubsection.3.3.4}%
\contentsline {subsubsection}{\numberline {3.3.5}Results Experiment 2}{16}{subsubsection.3.3.5}%
\contentsline {subsubsection}{\numberline {3.3.6}Results Experiment 3}{24}{subsubsection.3.3.6}%
\contentsline {subsubsection}{\numberline {3.3.7} Report results appropriately: use diagrams, report averages or medians and measurement errors, possibly use box plots. Graphical representations are preferred over large tables with many numbers. Describe what we see in the graph in your report text.}{31}{subsubsection.3.3.7}%
\contentsline {subsubsection}{\numberline {3.3.8}Interpret the results: explain why we see the results we see. Relate the results back to your architecture: how did your design decisions influence these results? If the results contradict your intuition, explain what caused this.}{31}{subsubsection.3.3.8}%
\contentsline {section}{\numberline {4}Insight Question}{31}{section.4}%
